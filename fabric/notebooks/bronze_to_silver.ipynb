{"cells":[{"cell_type":"code","source":["# Cell 1: Install package from GitHub\n","%pip install \"git+https://github.com/kpopov95-code/library-pipeline.git\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[10,11,12,13,14,15],"state":"finished","livy_statement_state":"available","session_id":"8c008e59-c68d-4cfd-97f6-0062a3cdd199","normalized_state":"finished","queued_time":"2025-11-12T15:00:35.7038416Z","session_start_time":null,"execution_start_time":"2025-11-12T15:00:40.2223427Z","execution_finish_time":"2025-11-12T15:01:06.0451126Z","parent_msg_id":"23a6e133-710e-42f1-a96e-a4cd2d387ff5"},"text/plain":"StatementMeta(, 8c008e59-c68d-4cfd-97f6-0062a3cdd199, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/kpopov95-code/library-pipeline.git\n  Cloning https://github.com/kpopov95-code/library-pipeline.git to /tmp/pip-req-build-2di219s3\n  Running command git clone --filter=blob:none --quiet https://github.com/kpopov95-code/library-pipeline.git /tmp/pip-req-build-2di219s3\n  Resolved https://github.com/kpopov95-code/library-pipeline.git to commit 1331b993ceedc34e0194943eac5bb38d7123e06e\n  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\n\u001b[?25hBuilding wheels for collected packages: library-pipeline\n  Building wheel for library-pipeline (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n\u001b[?25h  Created wheel for library-pipeline: filename=library_pipeline-0.1.0-py3-none-any.whl size=5464 sha256=978b231784b6e5b65194f59e6926699831d50214378f61efaf67bf5900720228\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5ujt13cd/wheels/92/ad/32/c9eea676a6e2e6bc75f93c24b3f2340818ced260481e8bfe24\nSuccessfully built library-pipeline\nInstalling collected packages: library-pipeline\nSuccessfully installed library-pipeline-0.1.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89c620ec-19f6-4bd3-8ed1-b3b6a8da38ce"},{"cell_type":"code","source":["from data_processing.ingestion import load_csv, load_json\n","from data_processing.cleaning import (\n","    remove_duplicates, \n","    handle_missing_values, \n","    standardize_dates\n",")\n","\n","print(\"✅ Package installed and imported successfully!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"8c008e59-c68d-4cfd-97f6-0062a3cdd199","normalized_state":"finished","queued_time":"2025-11-12T15:10:15.9231215Z","session_start_time":null,"execution_start_time":"2025-11-12T15:10:19.431183Z","execution_finish_time":"2025-11-12T15:10:19.7781008Z","parent_msg_id":"15a76f96-5f39-4b63-8645-faa5b515752e"},"text/plain":"StatementMeta(, 8c008e59-c68d-4cfd-97f6-0062a3cdd199, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Package installed and imported successfully!\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0bfe2611-6080-40fc-b0ec-cab1c7a38aa6"},{"cell_type":"code","source":["# Cell 3: Load data from Lakehouse Files\n","import pandas as pd\n","\n","# Read CSV from Files\n","file_path = \"/lakehouse/default/Files/bronze/circulation_data.csv\"\n","df_raw = pd.read_csv(file_path)\n","\n","print(f\"Loaded {len(df_raw)} rows\")\n","print(df_raw.head())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"8c008e59-c68d-4cfd-97f6-0062a3cdd199","normalized_state":"finished","queued_time":"2025-11-12T15:10:32.8475637Z","session_start_time":null,"execution_start_time":"2025-11-12T15:10:32.8486066Z","execution_finish_time":"2025-11-12T15:10:35.2247453Z","parent_msg_id":"afc324c1-938f-48d7-93bc-dde2be80d9e8"},"text/plain":"StatementMeta(, 8c008e59-c68d-4cfd-97f6-0062a3cdd199, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded 5100 rows\n  transaction_id member_id  ... return_date branch_id\n0      TXN000000    M93810  ...  2024-08-25     BR012\n1      TXN000001    M28289  ...  2024-09-02     BR011\n2      TXN000002    M21395  ...         NaN     BR001\n3      TXN000003    M38657  ...         NaN     BR010\n4      TXN000004    M36062  ...  2025-02-16     BR012\n\n[5 rows x 6 columns]\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2e0234a2-9c89-4fbc-8cea-a118c28d1814"},{"cell_type":"code","source":["# Cell 4: Apply your cleaning functions (BRONZE → SILVER)\n","print(\"Applying data cleaning pipeline...\")\n","\n","# Remove duplicates\n","df_clean = remove_duplicates(df_raw, subset=['transaction_id'])\n","print(f\"After removing duplicates: {len(df_clean)} rows\")\n","\n","# Handle missing values\n","df_clean = handle_missing_values(df_clean, strategy='drop')\n","print(f\"After handling missing values: {len(df_clean)} rows\")\n","\n","# Standardize dates\n","df_clean = standardize_dates(df_clean, ['checkout_date', 'return_date'])\n","print(\"Dates standardized\")\n","\n","print(f\"\\n✅ Cleaning complete! {len(df_raw)} → {len(df_clean)} rows\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"8c008e59-c68d-4cfd-97f6-0062a3cdd199","normalized_state":"finished","queued_time":"2025-11-12T15:10:57.9465783Z","session_start_time":null,"execution_start_time":"2025-11-12T15:10:57.9476685Z","execution_finish_time":"2025-11-12T15:10:58.3021861Z","parent_msg_id":"e5e9944b-6dae-41d9-82f3-7d7d4d5752f4"},"text/plain":"StatementMeta(, 8c008e59-c68d-4cfd-97f6-0062a3cdd199, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Applying data cleaning pipeline...\nAfter removing duplicates: 5000 rows\nAfter handling missing values: 4227 rows\nDates standardized\n\n✅ Cleaning complete! 5100 → 4227 rows\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0fb6c26e-1eb5-4900-b6b0-beab174a730d"},{"cell_type":"code","source":["# Cell 5: Save as Delta table (SILVER layer)\n","#from pyspark.sql import SparkSession\n","#spark = SparkSession.builder.getOrCreate()\n","\n","# Convert pandas to Spark DataFrame\n","df_spark = spark.createDataFrame(df_clean)\n","\n","# Write as Delta table\n","table_name = \"silver_circulation\"\n","df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n","\n","print(f\"✅ Created Delta table: {table_name}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"8c008e59-c68d-4cfd-97f6-0062a3cdd199","normalized_state":"finished","queued_time":"2025-11-12T15:12:13.1774336Z","session_start_time":null,"execution_start_time":"2025-11-12T15:12:13.1785292Z","execution_finish_time":"2025-11-12T15:12:42.305829Z","parent_msg_id":"7c890d96-d953-4cee-84f6-62cf1554c68f"},"text/plain":"StatementMeta(, 8c008e59-c68d-4cfd-97f6-0062a3cdd199, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Created Delta table: silver_circulation\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"52adb15d-69d3-4879-97c8-2ac2c03dd446"},{"cell_type":"code","source":["# Cell 6: Query the Delta table\n","query = f\"\"\"\n","SELECT \n","    COUNT(*) as total_transactions,\n","    COUNT(DISTINCT member_id) as unique_members,\n","    COUNT(DISTINCT isbn) as unique_books,\n","    COUNT(DISTINCT branch_id) as branches\n","FROM {table_name}\n","\"\"\"\n","\n","result = spark.sql(query)\n","result.show()\n","\n","print(\"✅ Silver layer ready for analysis!\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"8c008e59-c68d-4cfd-97f6-0062a3cdd199","normalized_state":"finished","queued_time":"2025-11-12T15:14:33.7471432Z","session_start_time":null,"execution_start_time":"2025-11-12T15:14:33.7482336Z","execution_finish_time":"2025-11-12T15:14:41.8740052Z","parent_msg_id":"72a45cec-b00f-4c12-991d-3599c45b18a3"},"text/plain":"StatementMeta(, 8c008e59-c68d-4cfd-97f6-0062a3cdd199, 21, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------------------+--------------+------------+--------+\n|total_transactions|unique_members|unique_books|branches|\n+------------------+--------------+------------+--------+\n|              4227|          4127|        4227|      30|\n+------------------+--------------+------------+--------+\n\n✅ Silver layer ready for analysis!\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"281ebd06-1768-48ed-b9f4-151c4a0f68a4"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"337716e0-608d-4eb8-a919-d3af6a0e07a7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"4af88a80-629c-4ef7-95de-3be792ac9ee6","known_lakehouses":[{"id":"4af88a80-629c-4ef7-95de-3be792ac9ee6"}],"default_lakehouse_name":"library_cleaning","default_lakehouse_workspace_id":"73414790-8d84-45a8-a55e-924d8af1e5a5"}}},"nbformat":4,"nbformat_minor":5}